{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "import numpy.random as rd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_kernel(x, xi):\n",
    "    return np.exp(-np.linalg.norm(x - xi)**2)\n",
    "\n",
    "def compute_kernel_matrix(X, Y):\n",
    "    l = X.shape[0]\n",
    "    r = Y.shape[0]\n",
    "    K = np.zeros((l, r))\n",
    "    for i in range(l):\n",
    "        for j in range(r):\n",
    "            K[i, j] = euclidean_kernel(X[i], Y[j])\n",
    "    return K\n",
    "\n",
    "def compute_alpha_star(Kmm, Knm, y, sigma_squared, nu):\n",
    "    m = Kmm.shape[0]\n",
    "    A = sigma_squared * Kmm + np.dot(Knm.T, Knm) + nu * np.eye(m)\n",
    "    b = np.dot(Knm.T, y)\n",
    "    alpha_star = np.linalg.solve(A, b)\n",
    "    return alpha_star\n",
    "    \n",
    "def local_objective_function(alpha, sigma_squared, K_mm, y_local, K_im, nu, a):\n",
    "    term1 = (sigma_squared / a) * (1 / 2) * np.dot(np.dot(alpha.T, K_mm), alpha)\n",
    "    term2 = (1 / 2) * np.sum((y_local - K_im @ alpha)**2)\n",
    "    term3 = (nu / (2 * a)) * np.linalg.norm(alpha)**2\n",
    "    return term1 + term2 + term3\n",
    "\n",
    "def compute_local_gradient(alpha, sigma_squared, K_mm, y_local, K_im, nu, a):\n",
    "    # print(\"alpha local \", alpha)\n",
    "    # print(\"K_mm @ alpha\",K_mm @ alpha)\n",
    "    grad = sigma_squared * K_mm @ alpha / a\n",
    "    # print(\"K_im.T.shape:\",K_im.T.shape)\n",
    "    # print(\"(K_im @ alpha - y_local).shape:\",(K_im @ alpha - y_local).shape)\n",
    "    # print(\"K_im.T @ (K_im @ alpha - y_local) \",K_im.T @ (K_im @ alpha - y_local))\n",
    "    grad += K_im.T @ (K_im @ alpha - y_local)\n",
    "    # print(\"grad.shape:\",grad.shape)\n",
    "    # print(\"alpha.shape:\",alpha.shape)\n",
    "    # print(\"(nu / a) * alpha\",(nu / a) * alpha)\n",
    "    grad += (nu / a) * alpha\n",
    "    return grad\n",
    "\n",
    "def Nystrom_approx_f(alpha,X_selected,X):\n",
    "    K1m = compute_kernel_matrix(X,X_selected)\n",
    "    # print(\"K1m\",K1m.shape)\n",
    "    # print(\"alpha.shape\",alpha.shape)\n",
    "    return K1m @ alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def generate_line_graph(a):\n",
    "    \"\"\"\n",
    "    Generate a line graph where each agent communicates only with its immediate neighbors.\n",
    "\n",
    "    Parameters:\n",
    "    a (int): Number of agents.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Adjacency matrix representing the line graph.\n",
    "    \"\"\"\n",
    "    line_graph = np.zeros((a, a))\n",
    "    for i in range(a - 1):\n",
    "        line_graph[i, i + 1] = 1\n",
    "        line_graph[i + 1, i] = 1\n",
    "    np.fill_diagonal(line_graph, 1)\n",
    "    return line_graph\n",
    "\n",
    "def generate_small_world_graph(a, p=0.1):\n",
    "    \"\"\"\n",
    "    Generate a small-world graph with a specified number of agents and probability of rewiring.\n",
    "\n",
    "    Parameters:\n",
    "    a (int): Number of agents.\n",
    "    p (float): Probability of rewiring.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Adjacency matrix representing the small-world graph.\n",
    "    \"\"\"\n",
    "    small_world_graph = nx.watts_strogatz_graph(a, 2, p)\n",
    "    small_world_graph=nx.to_numpy_array(small_world_graph)\n",
    "    np.fill_diagonal(small_world_graph, 1)\n",
    "    return small_world_graph\n",
    "\n",
    "def generate_fully_connected_graph(a):\n",
    "    \"\"\"\n",
    "    Generate a fully connected graph where each agent communicates with every other agent directly.\n",
    "\n",
    "    Parameters:\n",
    "    a (int): Number of agents.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Adjacency matrix representing the fully connected graph.\n",
    "    \"\"\"\n",
    "    return np.ones((a, a)) /a\n",
    "\n",
    "def generate_cycle_graph(a):\n",
    "    \"\"\"\n",
    "    Generate a cycle graph where each agent communicates with its two adjacent agents forming a cycle.\n",
    "\n",
    "    Parameters:\n",
    "    a (int): Number of agents.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Adjacency matrix representing the cycle graph.\n",
    "    \"\"\"\n",
    "    cycle_graph = np.zeros((a, a))\n",
    "    for i in range(a):\n",
    "        cycle_graph[i, (i - 1) % a] = 1  # Left neighbor\n",
    "        cycle_graph[i, (i + 1) % a] = 1  # Right neighbor\n",
    "    np.fill_diagonal(cycle_graph,1)\n",
    "    return cycle_graph\n",
    "\n",
    "def renormalize_graph(graph):\n",
    "    \"\"\"\n",
    "    Renormalize the given graph to make it doubly stochastic.\n",
    "\n",
    "    Parameters:\n",
    "    graph (np.ndarray): Adjacency matrix representing the graph.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Doubly stochastic matrix obtained by renormalizing the input graph.\n",
    "    \"\"\"\n",
    "    row_sums = np.sum(graph, axis=1)\n",
    "    #print(\"row_sums:\",row_sums)\n",
    "    graph = graph / row_sums[:, np.newaxis]\n",
    "    # print(\"row normalized graph\",graph)\n",
    "    col_sums = np.sum(graph, axis=0)\n",
    "    #print(\"col_sums:\",col_sums)\n",
    "    graph = graph / col_sums[np.newaxis,:]\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decentralized_gradient_descent(X, X_selected, y, sigma_squared, nu, a, num_iterations, step_size, alpha_star, W):\n",
    "    m = X_selected.shape[0]\n",
    "    n = X.shape[0]\n",
    "    agents_data_indices = np.array_split(np.random.permutation(n), a)\n",
    "    # each agent should have n/a=100/5=20 points\n",
    "    #print(\"agents_data_indices:\",agents_data_indices)\n",
    "    alpha = np.ones((m * a, 1))  # Initialize local variables for each agent\n",
    "    all_optimality_gaps = [[] for _ in range(a)]  # Store optimality gaps for each agent\n",
    "    all_alpha = []  # Store alpha at each iteration\n",
    "    \n",
    "    # Create mixing matrix from communication matrix\n",
    "    mixing_matrix = np.kron(W, np.eye(m))  # Mixing matrix\n",
    "    \n",
    "    # Compute kernel matrices\n",
    "    K_mm = compute_kernel_matrix(X_selected, X_selected)\n",
    "    #print(\"K_mm\",K_mm)\n",
    "    #Knm = compute_kernel_matrix(X, X_selected)\n",
    "    #print(\"Knm \",Knm)\n",
    "    \n",
    "    for iter in tqdm(range(num_iterations)):\n",
    "        #print(\"iter:\",iter)\n",
    "        \n",
    "        # Append current alpha to the list\n",
    "        all_alpha.append(alpha)\n",
    "        \n",
    "        all_gradients = np.zeros((m * a, 1))\n",
    "        \n",
    "        # Compute gradients for all agents\n",
    "        for agent_idx, data_indices in enumerate(agents_data_indices):\n",
    "            # Select data and compute local variables\n",
    "            X_local = X[data_indices]\n",
    "            y_local = y[data_indices].reshape(-1, 1)\n",
    "            K_im = compute_kernel_matrix(X_local, X_selected)\n",
    "            \n",
    "            # Compute local gradient on local copy of alpha\n",
    "            grad_local = compute_local_gradient(alpha[m * agent_idx: m * (agent_idx + 1)], sigma_squared, K_mm, y_local, K_im, nu, a)\n",
    "            all_gradients[m * agent_idx: m * (agent_idx + 1)] = grad_local\n",
    "        \n",
    "        # Update alpha simultaneously for all agents\n",
    "\n",
    "        alpha = mixing_matrix @ alpha - step_size * all_gradients\n",
    "        \n",
    "        # Compute and store optimality gaps for each agent\n",
    "        for agent_idx in range(a):\n",
    "            alpha_agent = alpha[m * agent_idx: m * (agent_idx + 1)].reshape(-1, 1)\n",
    "            optimality_gap = np.linalg.norm(alpha_agent - alpha_star.reshape(-1, 1))\n",
    "            all_optimality_gaps[agent_idx].append(optimality_gap)\n",
    "        \n",
    "    return all_optimality_gaps, all_alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decentralized_gradient_tracking(X, X_selected, y, sigma_squared, nu, a, num_iterations, step_size, alpha_star, W):\n",
    "    m = X_selected.shape[0]\n",
    "    n = X.shape[0]\n",
    "    agents_data_indices = np.array_split(np.random.permutation(n), a)\n",
    "    # each agent should have n/a=100/5=20 points\n",
    "    #print(\"agents_data_indices:\",agents_data_indices)\n",
    "    alpha = np.ones((m * a, 1))  # Initialize local variables for each agent\n",
    "    all_optimality_gaps = [[] for _ in range(a)]  # Store optimality gaps for each agent\n",
    "    all_alpha = []  # Store alpha at each iteration\n",
    "    gradient_tracker = np.zeros((m * a, 1))  # Initialize gradient tracker\n",
    "    \n",
    "    # Create mixing matrix from communication matrix\n",
    "    mixing_matrix = np.kron(W, np.eye(m))  # Mixing matrix\n",
    "    \n",
    "    # Compute kernel matrices\n",
    "    K_mm = compute_kernel_matrix(X_selected, X_selected)\n",
    "    #print(\"K_mm\",K_mm)\n",
    "    Knm = compute_kernel_matrix(X, X_selected)\n",
    "    #print(\"Knm \",Knm)\n",
    "    grad0 = np.zeros((m * a, 1))\n",
    "    grad1 =  np.zeros((m * a, 1)) #store the two last gradients for all agents\n",
    "    for iter in tqdm(range(num_iterations)):\n",
    "        #print(\"iter:\",iter)\n",
    "        \n",
    "        # Append current alpha to the list\n",
    "        all_alpha.append(alpha)\n",
    "        \n",
    "        all_gradients = np.zeros((m * a, 1))\n",
    "        \n",
    "        # Compute gradients for all agents\n",
    "        for agent_idx, data_indices in enumerate(agents_data_indices):\n",
    "            # Select data and compute local variables\n",
    "            X_local = X[data_indices]\n",
    "            y_local = y[data_indices].reshape(-1, 1)\n",
    "            K_im = compute_kernel_matrix(X_local, X_selected)\n",
    "            \n",
    "            # Compute local gradient on local copy of alpha\n",
    "            grad_local = compute_local_gradient(alpha[m * agent_idx: m * (agent_idx + 1)], sigma_squared, K_mm, y_local, K_im, nu, a)\n",
    "            all_gradients[m * agent_idx: m * (agent_idx + 1)] = grad_local\n",
    "        grad0 = grad1\n",
    "        grad1 = all_gradients\n",
    "         # Gradient tracking update\n",
    "        gradient_tracker = mixing_matrix @ gradient_tracker + grad1 - grad0\n",
    "        # Update alpha using the gradient tracker\n",
    "        alpha = mixing_matrix @ alpha - step_size * gradient_tracker\n",
    "        \n",
    "        # Compute and store optimality gaps for each agent\n",
    "        for agent_idx in range(a):\n",
    "            alpha_agent = alpha[m * agent_idx: m * (agent_idx + 1)].reshape(-1, 1)\n",
    "            optimality_gap = np.linalg.norm(alpha_agent - alpha_star.reshape(-1, 1))\n",
    "            all_optimality_gaps[agent_idx].append(optimality_gap)\n",
    "        \n",
    "    return all_optimality_gaps, all_alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(b, Kmm, Knm, Knmy, nu, sigma_squared):\n",
    "    m = Kmm.shape[0]\n",
    "    A = (1/5)*sigma_squared * Kmm + Knm + (nu/5) * np.eye(m)\n",
    "    b_ = Knmy - b\n",
    "    alpha_star_i = np.linalg.solve(A, b_)\n",
    "    return alpha_star_i\n",
    "\n",
    "def create_lamb(a, m):\n",
    "    lamb = {}\n",
    "    for i in range(a):\n",
    "        for j in range(a):\n",
    "            if i > j:\n",
    "                #lamb[(i,j)] = np.random.rand(m).reshape(m,1)\n",
    "                lamb[(i,j)] = np.zeros(m).reshape(m,1)\n",
    "    return(lamb)\n",
    "\n",
    "def term_sup(lam, W, m, i):\n",
    "    a = W.shape[0]\n",
    "    res = np.zeros((m,1))\n",
    "    for j in range(a):\n",
    "        if W[i,j] > 0:\n",
    "            if i > j:\n",
    "                res += lam[i,j]\n",
    "            elif j > i:\n",
    "                res -=  lam[j,i]\n",
    "    return(res)\n",
    "\n",
    "def dual_decomposition(X, X_selected, y, sigma_squared, nu, a, num_iterations, step_size, alpha_star, W):\n",
    "    m = X_selected.shape[0]\n",
    "    n = X.shape[0]\n",
    "    agents_data_indices = np.array_split(np.random.permutation(n), a)\n",
    "    # each agent should have n/a=100/5=20 points\n",
    "    #print(\"agents_data_indices:\",agents_data_indices)\n",
    "    alpha = [np.zeros((m, 1)) for _ in range(a)]  # Initialize local variables for each agent\n",
    "    all_optimality_gaps = [[] for _ in range(a)]  # Store optimality gaps for each agent\n",
    "    all_alpha = [[] for _ in range(a)]  # Store alpha at each iteration\n",
    "    lamb = create_lamb(a, m)\n",
    "    K_mm = compute_kernel_matrix(X_selected, X_selected)\n",
    "    #Knm = compute_kernel_matrix(X, X_selected)\n",
    "    for k in tqdm(range(num_iterations)):\n",
    "        for agent_idx, data_indices in enumerate(agents_data_indices):\n",
    "            # Select data and compute local variables\n",
    "            X_local = X[data_indices]\n",
    "            y_local = y[data_indices].reshape(-1, 1)\n",
    "            K_im = np.zeros((m,m))\n",
    "            K_imy = np.zeros((m,1))\n",
    "            for j in range(len(X_local)):\n",
    "                K_jm = compute_kernel_matrix(np.array([X_local[j]]), X_selected)\n",
    "                K_jmy = y_local[j]*K_jm.T\n",
    "                KK_im = np.dot(K_jm.T, K_jm)\n",
    "                K_im += KK_im\n",
    "                K_imy += K_jmy\n",
    "            b = term_sup(lamb, W, m, agent_idx)\n",
    "            #print(b)\n",
    "            sol = step(b, K_mm, K_im, K_imy, nu, sigma_squared)\n",
    "            alpha[agent_idx] = sol\n",
    "        for i in range(a):\n",
    "            for j in range(i):\n",
    "                lamb[i,j] += step_size[k]*(alpha[i] - alpha[j])\n",
    "        for agent_idx in range(a):\n",
    "            alpha_agent = alpha[agent_idx]\n",
    "            optimality_gap = np.linalg.norm(alpha_agent - alpha_star.reshape(-1, 1))\n",
    "            all_optimality_gaps[agent_idx].append(optimality_gap)\n",
    "            all_alpha[agent_idx].append(alpha_agent)\n",
    "\n",
    "    return all_optimality_gaps, all_alpha\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lamb_gam(a, m):\n",
    "    lamb = {}\n",
    "    gam = {}\n",
    "    for i in range(a):\n",
    "        for j in range(a):\n",
    "            lamb[(i,j)] = np.zeros(m).reshape(m,1)\n",
    "            gam[(i,j)] =  np.random.rand(m).reshape(m,1)\n",
    "    return(lamb, gam)\n",
    "\n",
    "def step2(b,c, Kmm, Knm,Knmy, nu, sigma_squared):\n",
    "    m = Kmm.shape[0]\n",
    "    A = (1/5)*sigma_squared * Kmm + Knm + ((nu/5) +c) * np.eye(m)\n",
    "    b_ = Knmy - b\n",
    "    alpha_star_i = np.linalg.solve(A, b_)\n",
    "    return alpha_star_i\n",
    "\n",
    "def term_sup2(gam, lamb, beta, W, i):\n",
    "    c = 0\n",
    "    b = 0\n",
    "    a = W.shape[0]\n",
    "    for j in range(a):\n",
    "        if W[i,j] > 0:\n",
    "            c += beta\n",
    "            b += lamb[i,j] - beta*gam[i,j]\n",
    "    return b, c\n",
    "\n",
    "def ADMM(X, X_selected, y, sigma_squared, nu, a, num_iterations, beta, alpha_star, W):\n",
    "    m = X_selected.shape[0]\n",
    "    n = X.shape[0]\n",
    "    agents_data_indices = np.array_split(np.random.permutation(n), a)\n",
    "    # each agent should have n/a=100/5=20 points\n",
    "    #print(\"agents_data_indices:\",agents_data_indices)\n",
    "    alpha = [np.ones((m, 1)) for _ in range(a)]  # Initialize local variables for each agent\n",
    "    all_optimality_gaps = [[] for _ in range(a)]  # Store optimality gaps for each agent\n",
    "    all_alpha = [[] for _ in range(a)]  # Store alpha at each iteration\n",
    "    lamb, gam = create_lamb_gam(a, m)\n",
    "    K_mm = compute_kernel_matrix(X_selected, X_selected)\n",
    "    #Knm = compute_kernel_matrix(X, X_selected)\n",
    "    for _ in tqdm(range(num_iterations)):\n",
    "        for agent_idx, data_indices in enumerate(agents_data_indices):\n",
    "            # Select data and compute local variables\n",
    "            X_local = X[data_indices]\n",
    "            y_local = y[data_indices].reshape(-1, 1)\n",
    "            K_im = np.zeros((m,m))\n",
    "            K_imy = np.zeros((m,1))\n",
    "            for j in range(len(X_local)):\n",
    "                K_jm = compute_kernel_matrix(np.array([X_local[j]]), X_selected)\n",
    "                K_jmy = y_local[j]*K_jm.T\n",
    "                KK_im = np.dot(K_jm.T, K_jm)\n",
    "                K_im += KK_im\n",
    "                K_imy += K_jmy\n",
    "            b, c = term_sup2(gam, lamb,beta, W, agent_idx)\n",
    "            sol = step2(b,c, K_mm, K_im, K_imy, nu, sigma_squared)\n",
    "            alpha[agent_idx] = sol\n",
    "        for i in range(a):\n",
    "            for j in range(i):\n",
    "                gam[i,j] = (1/2)*(alpha[i] + alpha[j])\n",
    "                lamb[i,j] += beta*(alpha[i] - gam[i,j])\n",
    "        for agent_idx in range(a):\n",
    "            alpha_agent = alpha[agent_idx]\n",
    "            optimality_gap = np.linalg.norm(alpha_agent - alpha_star.reshape(-1, 1))\n",
    "            all_optimality_gaps[agent_idx].append(optimality_gap)\n",
    "            all_alpha[agent_idx].append(alpha_agent)\n",
    "\n",
    "    return all_optimality_gaps, all_alpha"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cc7739b244f974916000a7d8eb95217d7b293019de62b65f8f3aaabbca2f3ce9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
